"""ë©€í‹°í„´ ë¦¬ë·° ì‹¤í–‰ê¸°"""

import concurrent.futures

from selvage.src.llm_gateway.base_gateway import BaseGateway
from selvage.src.models.review_result import ReviewResult
from selvage.src.utils.prompts.models import (
    ReviewPromptWithFileContent,
    SystemPrompt,
    UserPromptWithFileContent,
)
from selvage.src.utils.token.models import EstimatedCost, ReviewResponse
from selvage.src.utils.token.token_utils import TokenUtils

from .models import TokenInfo
from .prompt_splitter import PromptSplitter


class MultiturnReviewExecutor:
    """Multiturn Review ë©”ì¸ ì‹¤í–‰ê¸°"""

    def __init__(self) -> None:
        self.prompt_splitter = PromptSplitter()

    def execute_multiturn_review(
        self,
        review_prompt: ReviewPromptWithFileContent,
        token_info: TokenInfo,
        llm_gateway: BaseGateway,
    ) -> ReviewResult:
        """
        Context limit ì´ˆê³¼ ì‹œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„í• í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ í›„ ê²°ê³¼ í•©ì„±

        Args:
            review_prompt: ì´ë¯¸ ìƒì„±ëœ ë¦¬ë·° í”„ë¡¬í”„íŠ¸ (cli.py:L383ì—ì„œ ìƒì„±)
            token_info: í† í° ì •ë³´ (actual_tokens, max_tokens)
            llm_gateway: LLM API í˜¸ì¶œ ê²Œì´íŠ¸ì›¨ì´

        Returns:
            ReviewResult: í•©ì„±ëœ ìµœì¢… ë¦¬ë·° ê²°ê³¼
        """
        # ë¹ˆ user_prompts ì²˜ë¦¬
        if not review_prompt.user_prompts:
            return ReviewResult.get_empty_result(llm_gateway.get_model_name())

        # ë””ë²„ê¹…: ì…ë ¥ íŒŒë¼ë¯¸í„° ë¡œê¹…
        print("ğŸ” [DEBUG] MultiturnReviewExecutor ì…ë ¥:")
        print(f"   - Model: {llm_gateway.get_model_name()}")
        print(f"   - Provider: {llm_gateway.get_provider().value}")
        print(f"   - actual_tokens: {token_info.actual_tokens:,}")
        print(f"   - max_tokens: {token_info.max_tokens:,}")
        print(f"   - user_prompts count: {len(review_prompt.user_prompts)}")

        # 1. user_prompts ë¶„í•  (system_promptëŠ” ê³µí†µ ì‚¬ìš©)
        user_prompt_chunks = self.prompt_splitter.split_user_prompts(
            user_prompts=review_prompt.user_prompts,
            actual_tokens=token_info.actual_tokens,
            max_tokens=token_info.max_tokens,
            overlap=0,
        )

        # ë””ë²„ê¹…: ë¶„í•  ê²°ê³¼ ë¡œê¹…
        print("ğŸ” [DEBUG] ë¶„í•  ê²°ê³¼:")
        print(f"   - ì´ ì²­í¬ ê°œìˆ˜: {len(user_prompt_chunks)}")
        for i, chunk in enumerate(user_prompt_chunks):
            print(f"   - ì²­í¬ {i}: {len(chunk)} user_prompts")

            # ê° ì²­í¬ì˜ ì‹¤ì œ í† í° ìˆ˜ ê³„ì‚°
            chunk_prompt = ReviewPromptWithFileContent(
                system_prompt=review_prompt.system_prompt, user_prompts=chunk
            )
            chunk_tokens = TokenUtils.count_tokens(
                chunk_prompt, llm_gateway.get_model_name()
            )
            print(f"     â†’ ì‹¤ì œ í† í° ìˆ˜: {chunk_tokens:,}")
            status_text = "ì´ˆê³¼" if chunk_tokens > token_info.max_tokens else "ì•ˆì „"
            print(f"     -> {token_info.max_tokens} í•œê³„ {status_text}")
        print()

        # 2. ìˆœì°¨ API í˜¸ì¶œ (OpenRouter ë™ì‹œì„± ë¬¸ì œ í•´ê²°)
        review_results = self._execute_sequential_reviews(
            user_prompt_chunks, review_prompt.system_prompt, llm_gateway
        )

        # 3. ê²°ê³¼ ê°„ë‹¨ ë³‘í•©
        merged_result = self._merge_review_results(review_results)

        return merged_result

    def _execute_parallel_reviews(
        self,
        user_prompt_chunks: list[list[UserPromptWithFileContent]],
        system_prompt: SystemPrompt,
        llm_gateway: BaseGateway,
    ) -> list[ReviewResult]:
        """ë¶„í• ëœ ì²­í¬ë“¤ì— ëŒ€í•œ ë³‘ë ¬ API í˜¸ì¶œ"""
        review_results: list[ReviewResult] = []

        # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            # ê° ì²­í¬ì— ëŒ€í•´ ë¦¬ë·° ìš”ì²­ ìƒì„±
            future_to_chunk = {}
            for chunk in user_prompt_chunks:
                chunk_review_prompt = ReviewPromptWithFileContent(
                    system_prompt=system_prompt, user_prompts=chunk
                )
                future = executor.submit(llm_gateway.review_code, chunk_review_prompt)
                future_to_chunk[future] = chunk

            # ê²°ê³¼ ìˆ˜ì§‘
            for future in concurrent.futures.as_completed(future_to_chunk):
                try:
                    result = future.result()
                    review_results.append(result)
                except Exception as e:
                    # ê°œë³„ ì²­í¬ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ê²°ê³¼ ìƒì„±
                    error_result = ReviewResult.get_error_result(
                        error=e,
                        model=llm_gateway.get_model_name(),
                        provider=llm_gateway.get_provider().value,
                    )
                    review_results.append(error_result)

        return review_results

    def _execute_sequential_reviews(
        self,
        user_prompt_chunks: list[list[UserPromptWithFileContent]],
        system_prompt: SystemPrompt,
        llm_gateway: BaseGateway,
    ) -> list[ReviewResult]:
        """ë¶„í• ëœ ì²­í¬ë“¤ì— ëŒ€í•œ ìˆœì°¨ API í˜¸ì¶œ (OpenRouter ë™ì‹œì„± ë¬¸ì œ í•´ê²°)"""
        review_results: list[ReviewResult] = []

        print(f"ğŸ”„ [DEBUG] ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘: {len(user_prompt_chunks)}ê°œ ì²­í¬")

        for i, chunk in enumerate(user_prompt_chunks):
            print(f"ğŸ”„ [DEBUG] ì²­í¬ {i + 1}/{len(user_prompt_chunks)} ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘")

            chunk_review_prompt = ReviewPromptWithFileContent(
                system_prompt=system_prompt, user_prompts=chunk
            )

            try:
                result = llm_gateway.review_code(chunk_review_prompt)
                review_results.append(result)
                print(f"âœ… [DEBUG] ì²­í¬ {i + 1} ì²˜ë¦¬ ì„±ê³µ")
            except Exception as e:
                # ê°œë³„ ì²­í¬ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ê²°ê³¼ ìƒì„±
                error_result = ReviewResult.get_error_result(
                    error=e,
                    model=llm_gateway.get_model_name(),
                    provider=llm_gateway.get_provider().value,
                )
                review_results.append(error_result)
                print(f"âŒ [DEBUG] ì²­í¬ {i + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

        print(f"ğŸ”„ [DEBUG] ìˆœì°¨ ì²˜ë¦¬ ì™„ë£Œ: {len(review_results)}ê°œ ê²°ê³¼")
        return review_results

    def _merge_review_results(self, review_results: list[ReviewResult]) -> ReviewResult:
        """CR-18 êµ¬í˜„ì„ ìœ„í•œ **ìµœì†Œ ê¸°ëŠ¥ êµ¬í˜„(MVP)**ì´ë©°,
        CR-19ì—ì„œ ReviewSynthesizerë¡œ ì™„ì „íˆ êµì²´ë  ì˜ˆì •
        """
        if not review_results:
            return ReviewResult.get_empty_result("unknown")

        # ì‹¤íŒ¨í•œ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
        failed_results = [r for r in review_results if not r.success]
        if failed_results:
            # ì²« ë²ˆì§¸ ì‹¤íŒ¨ ê²°ê³¼ë¥¼ ë°˜í™˜
            return failed_results[0]

        # ì„±ê³µí•œ ê²°ê³¼ë“¤ì„ í•©ì„±
        successful_results = [r for r in review_results if r.success]
        if not successful_results:
            return ReviewResult.get_empty_result("unknown")

        # ë‚´ìš© í•©ì„±
        merged_summary_parts = []
        all_recommendations = []
        all_issues = []
        total_cost = EstimatedCost.get_zero_cost("merged")

        for result in successful_results:
            if result.review_response.summary:
                merged_summary_parts.append(result.review_response.summary)
            all_recommendations.extend(result.review_response.recommendations)
            all_issues.extend(result.review_response.issues)

            # ë¹„ìš© í•©ì‚°
            total_cost = EstimatedCost(
                model=result.estimated_cost.model,
                input_cost_usd=total_cost.input_cost_usd
                + result.estimated_cost.input_cost_usd,
                output_cost_usd=total_cost.output_cost_usd
                + result.estimated_cost.output_cost_usd,
                total_cost_usd=total_cost.total_cost_usd
                + result.estimated_cost.total_cost_usd,
                input_tokens=total_cost.input_tokens
                + result.estimated_cost.input_tokens,
                output_tokens=total_cost.output_tokens
                + result.estimated_cost.output_tokens,
            )

        # í•©ì„±ëœ ReviewResponse ìƒì„±
        merged_review_response = ReviewResponse(
            summary="\n\n".join(merged_summary_parts) if merged_summary_parts else "",
            recommendations=all_recommendations,
            issues=all_issues,
            score=successful_results[0].review_response.score,
        )

        return ReviewResult.get_success_result(
            review_response=merged_review_response, estimated_cost=total_cost
        )
