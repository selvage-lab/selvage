{
  "analysis_timestamp": "2025-08-07T02:08:18.384289",
  "total_results": 6,
  "results": [
    {
      "provider": "openai",
      "model": "gpt-4o",
      "context_limit": 128000,
      "error_type": "BadRequestError",
      "error_code": "context_length_exceeded",
      "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 273619 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
      "http_status_code": 400,
      "raw_error_data": {
        "error": {
          "message": "This model's maximum context length is 128000 tokens. However, your messages resulted in 273619 tokens. Please reduce the length of the messages.",
          "type": "invalid_request_error",
          "param": "messages",
          "code": "context_length_exceeded"
        },
        "message": "This model's maximum context length is 128000 tokens. However, your messages resulted in 273619 tokens. Please reduce the length of the messages.",
        "type": "invalid_request_error",
        "param": "messages",
        "code": "context_length_exceeded"
      },
      "timestamp": "2025-08-07T02:08:06.722141"
    },
    {
      "provider": "anthropic",
      "model": "claude-sonnet-4-20250514",
      "context_limit": 200000,
      "error_type": "BadRequestError",
      "error_code": "invalid_request_error",
      "error_message": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'prompt is too long: 209924 tokens > 200000 maximum'}}",
      "http_status_code": 400,
      "raw_error_data": {
        "type": "error",
        "error": {
          "type": "invalid_request_error",
          "message": "prompt is too long: 209924 tokens > 200000 maximum"
        },
        "contains_token_info": true
      },
      "timestamp": "2025-08-07T02:08:07.656854"
    },
    {
      "provider": "google",
      "model": "gemini-2.5-flash",
      "context_limit": 1048576,
      "error_type": "ClientError",
      "error_code": 429,
      "error_message": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_paid_tier_input_token_count', 'quotaId': 'GenerateContentPaidTierInputTokensPerModelPerMinute', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '1000000'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '47s'}]}}",
      "http_status_code": 429,
      "raw_error_data": {
        "error": {
          "code": 429,
          "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.",
          "status": "RESOURCE_EXHAUSTED",
          "details": [
            {
              "@type": "type.googleapis.com/google.rpc.QuotaFailure",
              "violations": [
                {
                  "quotaMetric": "generativelanguage.googleapis.com/generate_content_paid_tier_input_token_count",
                  "quotaId": "GenerateContentPaidTierInputTokensPerModelPerMinute",
                  "quotaDimensions": {
                    "location": "global",
                    "model": "gemini-2.5-flash"
                  },
                  "quotaValue": "1000000"
                }
              ]
            },
            {
              "@type": "type.googleapis.com/google.rpc.Help",
              "links": [
                {
                  "description": "Learn more about Gemini API quotas",
                  "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
                }
              ]
            },
            {
              "@type": "type.googleapis.com/google.rpc.RetryInfo",
              "retryDelay": "47s"
            }
          ]
        },
        "quota_exceeded": true,
        "google_quota_details": [
          {
            "@type": "type.googleapis.com/google.rpc.QuotaFailure",
            "violations": [
              {
                "quotaMetric": "generativelanguage.googleapis.com/generate_content_paid_tier_input_token_count",
                "quotaId": "GenerateContentPaidTierInputTokensPerModelPerMinute",
                "quotaDimensions": {
                  "location": "global",
                  "model": "gemini-2.5-flash"
                },
                "quotaValue": "1000000"
              }
            ]
          },
          {
            "@type": "type.googleapis.com/google.rpc.Help",
            "links": [
              {
                "description": "Learn more about Gemini API quotas",
                "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
              }
            ]
          },
          {
            "@type": "type.googleapis.com/google.rpc.RetryInfo",
            "retryDelay": "47s"
          }
        ]
      },
      "timestamp": "2025-08-07T02:08:10.430208"
    },
    {
      "provider": "openrouter",
      "model": "qwen3-coder",
      "context_limit": 1000000,
      "error_type": "HTTPStatusError",
      "error_code": 400,
      "error_message": "Client error '400 Bad Request' for url 'https://openrouter.ai/api/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400",
      "http_status_code": 400,
      "raw_error_data": {
        "error": {
          "message": "This endpoint's maximum context length is 1000000 tokens. However, you requested about 2315418 tokens (2315418 of text input). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.",
          "code": 400,
          "metadata": {
            "provider_name": null
          }
        }
      },
      "timestamp": "2025-08-07T02:08:16.612664"
    },
    {
      "provider": "openrouter",
      "model": "kimi-k2",
      "context_limit": 131000,
      "error_type": "HTTPStatusError",
      "error_code": 400,
      "error_message": "Client error '400 Bad Request' for url 'https://openrouter.ai/api/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400",
      "http_status_code": 400,
      "raw_error_data": {
        "error": {
          "message": "This endpoint's maximum context length is 131072 tokens. However, you requested about 303521 tokens (303521 of text input). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.",
          "code": 400,
          "metadata": {
            "provider_name": null
          }
        }
      },
      "timestamp": "2025-08-07T02:08:17.226250"
    },
    {
      "provider": "openrouter",
      "model": "deepseek-r1-0528",
      "context_limit": 163840,
      "error_type": "HTTPStatusError",
      "error_code": 400,
      "error_message": "Client error '400 Bad Request' for url 'https://openrouter.ai/api/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400",
      "http_status_code": 400,
      "raw_error_data": {
        "error": {
          "message": "This endpoint's maximum context length is 163840 tokens. However, you requested about 379537 tokens (379537 of text input). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.",
          "code": 400,
          "metadata": {
            "provider_name": null
          }
        }
      },
      "timestamp": "2025-08-07T02:08:18.382266"
    }
  ]
}
