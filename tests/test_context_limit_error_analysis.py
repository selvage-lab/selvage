"""
Context Limit ì—ëŸ¬ ì§ì ‘ API ì‘ë‹µ ë¶„ì„ í…ŒìŠ¤íŠ¸

ê° LLM providerë³„ë¡œ context limitì„ ì´ˆê³¼í–ˆì„ ë•Œ ì‹¤ì œ APIì—ì„œ ë°˜í™˜ë˜ëŠ”
ì›ë³¸ ì—ëŸ¬ ì‘ë‹µì„ ì§ì ‘ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•˜ê¸° ìœ„í•œ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
    pytest tests/test_context_limit_error_analysis.py -v -s

ì£¼ì˜ì‚¬í•­:
    - ì‹¤ì œ APIë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    - ê° providerì˜ API keyê°€ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
"""

import json
import os
from dataclasses import asdict, dataclass
from datetime import datetime
from pathlib import Path
from typing import Any

import pytest
import yaml

from selvage.src.llm_gateway.gateway_factory import GatewayFactory
from selvage.src.model_config import ModelInfoDict, ModelProvider


@dataclass
class ContextLimitErrorAnalysis:
    """Context limit ì—ëŸ¬ ë¶„ì„ ê²°ê³¼"""

    provider: str
    model: str
    context_limit: int
    error_type: str
    error_code: str | None
    error_message: str
    http_status_code: int | None
    raw_error_data: dict[str, Any]
    timestamp: str


class ContextLimitTester:
    """Context limit ì—ëŸ¬ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í´ë˜ìŠ¤"""

    def __init__(self) -> None:
        self.results: list[ContextLimitErrorAnalysis] = []
        self.results_dir = Path("tests/results")
        self.results_dir.mkdir(exist_ok=True)

    def create_oversized_messages(
        self, context_limit_tokens: int
    ) -> list[dict[str, Any]]:
        """Context limitì„ í™•ì‹¤íˆ ì´ˆê³¼í•˜ëŠ” ë§¤ìš° ê¸´ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # context limitì˜ 200% ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í™•ì‹¤íˆ ì´ˆê³¼
        target_tokens = int(context_limit_tokens * 2.0)

        # ëŒ€ëµì ì¸ í† í° ê³„ì‚°: í‰ê· ì ìœ¼ë¡œ 1 í† í° = 4 ê¸€ìë¡œ ê°€ì •
        chars_per_token = 4
        target_chars = target_tokens * chars_per_token

        # ë§¤ìš° ê¸´ ì½”ë“œ ë‚´ìš© ìƒì„±
        base_code = """def complex_function_{index}(a, b, c, d, e, f):
    '''ë§¤ìš° ë³µì¡í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤'''
    result = 0
    for i in range(1000):
        temp = a * b + c * d - e * f
        result += temp * i
        if result > 10000:
            result = result % 10000
    return result

class DataProcessor_{index}:
    def __init__(self, data):
        self.data = data
        self.results = []
    
    def process(self):
        for item in self.data:
            processed = self.transform(item)
            self.results.append(processed)
    
    def transform(self, item):
        return {{
            'value': item.get('value', 0) * 2,
            'name': item.get('name', '').upper(),
            'processed_at': 'now'
        }}

"""

        # ê¸´ ì½”ë“œ ë‚´ìš© ìƒì„±
        long_code = ""
        index = 0
        while len(long_code) < target_chars:
            long_code += base_code.format(index=index)
            index += 1

        return [
            {
                "role": "system",
                "content": "ë‹¤ìŒ ì½”ë“œë¥¼ ìì„¸íˆ ë¦¬ë·°í•´ì£¼ì„¸ìš”. ëª¨ë“  í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ì— ëŒ€í•´ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
            },
            {
                "role": "user",
                "content": f"ë‹¤ìŒì€ ë¶„ì„í•  ì½”ë“œì…ë‹ˆë‹¤:\n\n```python\n{long_code}\n```\n\nìœ„ ì½”ë“œì— ëŒ€í•´ ìƒì„¸í•œ ë¦¬ë·°ë¥¼ í•´ì£¼ì„¸ìš”.",
            },
        ]

    def create_anthropic_oversized_content(
        self, context_limit_tokens: int
    ) -> tuple[str, list[dict[str, Any]]]:
        """Anthropic APIìš© systemê³¼ messagesë¥¼ ë¶„ë¦¬í•˜ì—¬ ìƒì„±í•©ë‹ˆë‹¤."""
        # context limitì˜ 200% ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ì •
        target_tokens = int(context_limit_tokens * 2.0)
        chars_per_token = 4
        target_chars = target_tokens * chars_per_token

        # ê¸´ ì½”ë“œ ë‚´ìš© ìƒì„±
        base_code = """def complex_function_{index}(a, b, c, d, e, f):
    '''ë§¤ìš° ë³µì¡í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤'''
    result = 0
    for i in range(1000):
        temp = a * b + c * d - e * f
        result += temp * i
        if result > 10000:
            result = result % 10000
    return result

class DataProcessor_{index}:
    def __init__(self, data):
        self.data = data
        self.results = []

    def process(self):
        for item in self.data:
            processed = self.transform(item)
            self.results.append(processed)

    def transform(self, item):
        return {{
            'value': item.get('value', 0) * 2,
            'name': item.get('name', '').upper(),
            'processed_at': 'now'
        }}

"""

        long_code = ""
        index = 0
        while len(long_code) < target_chars:
            long_code += base_code.format(index=index)
            index += 1

        system_content = (
            "ë‹¤ìŒ ì½”ë“œë¥¼ ìì„¸íˆ ë¦¬ë·°í•´ì£¼ì„¸ìš”. "
            "ëª¨ë“  í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ì— ëŒ€í•´ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."
        )

        messages = [
            {
                "role": "user",
                "content": (
                    f"ë‹¤ìŒì€ ë¶„ì„í•  ì½”ë“œì…ë‹ˆë‹¤:\n\n```python\n{long_code}\n```\n\n"
                    "ìœ„ ì½”ë“œì— ëŒ€í•´ ìƒì„¸í•œ ë¦¬ë·°ë¥¼ í•´ì£¼ì„¸ìš”."
                ),
            }
        ]

        return system_content, messages

    def analyze_error(
        self, provider: str, model: str, context_limit: int, error: Exception
    ) -> ContextLimitErrorAnalysis:
        """ì—ëŸ¬ë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        error_type = type(error).__name__
        error_message = str(error)
        error_code = None
        http_status = None
        raw_data = {}

        # OpenAI ì—ëŸ¬ ì²˜ë¦¬
        if hasattr(error, "response") and hasattr(error.response, "status_code"):
            http_status = error.response.status_code
            try:
                raw_data = error.response.json()
                if "error" in raw_data:
                    error_info = raw_data["error"]
                    if isinstance(error_info, dict):
                        error_code = error_info.get("code") or error_info.get("type")
            except Exception as json_error:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ë¡œê¹…
                print(f"   JSON íŒŒì‹± ì‹¤íŒ¨: {json_error}")

        # Anthropic ì—ëŸ¬ ì²˜ë¦¬
        if hasattr(error, "status_code"):
            http_status = error.status_code
        if hasattr(error, "type"):
            error_code = error.type

        # Anthropic ì—ëŸ¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        if hasattr(error, "body") and isinstance(error.body, dict):
            raw_data.update(error.body)
            if "error" in error.body:
                error_info = error.body["error"]
                if isinstance(error_info, dict):
                    error_code = error_info.get("type") or error_code
                    # í† í° ì •ë³´ë‚˜ context limit ì •ë³´ ì¶”ì¶œ ì‹œë„
                    error_msg = error_info.get("message", "")
                    if "tokens" in error_msg.lower():
                        raw_data["contains_token_info"] = True

        # ì¶”ê°€ ì—ëŸ¬ ì •ë³´ ìˆ˜ì§‘
        if hasattr(error, "code"):
            error_code = error.code

        # OpenAI/OpenRouter ì—ëŸ¬ì—ì„œ í† í° ì •ë³´ ì¶”ì¶œ
        if "tokens" in error_message.lower() and provider in ["openai", "openrouter"]:
            # ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ì‹¤ì œ í† í° ìˆ˜ì™€ ìµœëŒ€ í† í° ìˆ˜ ì¶”ì¶œ ì‹œë„
            import re

            token_match = re.search(
                r"(\d+,?\d*)\s+tokens.*maximum.*?(\d+,?\d*)\s+tokens", error_message
            )
            if token_match:
                raw_data["actual_tokens"] = token_match.group(1).replace(",", "")
                raw_data["max_tokens"] = token_match.group(2).replace(",", "")

        # Google ì—ëŸ¬ì—ì„œ quota ì •ë³´ ì¶”ì¶œ
        if provider == "google" and "quota" in error_message.lower():
            raw_data["quota_exceeded"] = True
            if hasattr(error, "response") and hasattr(error.response, "json"):
                try:
                    google_error_data = error.response.json()
                    if (
                        "error" in google_error_data
                        and "details" in google_error_data["error"]
                    ):
                        raw_data["google_quota_details"] = google_error_data["error"][
                            "details"
                        ]
                except Exception as json_error:
                    print(f"   Google JSON íŒŒì‹± ì‹¤íŒ¨: {json_error}")

        return ContextLimitErrorAnalysis(
            provider=provider,
            model=model,
            context_limit=context_limit,
            error_type=error_type,
            error_code=error_code,
            error_message=error_message,
            http_status_code=http_status,
            raw_error_data=raw_data,
            timestamp=datetime.now().isoformat(),
        )

    def save_results(self):
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        if not self.results:
            print("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"context_limit_errors_{timestamp}.json"
        filepath = self.results_dir / filename

        results_dict = {
            "analysis_timestamp": datetime.now().isoformat(),
            "total_results": len(self.results),
            "results": [asdict(result) for result in self.results],
        }

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(results_dict, f, indent=2, ensure_ascii=False)

        print(f"\në¶„ì„ ê²°ê³¼ ì €ì¥ë¨: {filepath}")
        print(f"ì´ {len(self.results)}ê°œì˜ ì—ëŸ¬ ì‘ë‹µ ë¶„ì„ ì™„ë£Œ")


# ì „ì—­ í…ŒìŠ¤í„° ì¸ìŠ¤í„´ìŠ¤
tester = ContextLimitTester()


@pytest.fixture
def models_config():
    """models.ymlì—ì„œ ëª¨ë¸ ì„¤ì •ì„ ë¡œë“œí•˜ëŠ” fixture"""
    models_yml_path = (
        Path(__file__).parent.parent / "selvage" / "resources" / "models.yml"
    )

    with open(models_yml_path, encoding="utf-8") as f:
        config = yaml.safe_load(f)

    return config["models"]


def test_openai_context_limit_error():
    """OpenAI APIì˜ context limit ì—ëŸ¬ë¥¼ ì§ì ‘ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    if not os.getenv("OPENAI_API_KEY"):
        pytest.skip("OPENAI_API_KEY not found")

    model_info: ModelInfoDict = {
        "full_name": "gpt-4o",
        "aliases": [],
        "description": "Context limit test",
        "provider": ModelProvider.OPENAI,
        "params": {"temperature": 0.0},
        "thinking_mode": False,
        "pricing": {"input": 2.5, "output": 10.0, "description": "Test"},
        "context_limit": 128000,
    }

    print("\n=== OpenAI Context Limit í…ŒìŠ¤íŠ¸ ===")
    print(f"ëª¨ë¸: {model_info['full_name']}")
    print(f"Context Limit: {model_info['context_limit']:,} tokens")

    try:
        gateway = GatewayFactory.create(model_info["full_name"])
        messages = tester.create_oversized_messages(model_info["context_limit"])

        # ì§ì ‘ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° í˜¸ì¶œ
        client = gateway._create_client()
        params = gateway._create_request_params(messages)

        print(f"í”„ë¡¬í”„íŠ¸ í¬ê¸°: ì•½ {len(str(messages)) // 4:,} tokens ì¶”ì •")

        # ì§ì ‘ API í˜¸ì¶œ (Instructor í´ë¼ì´ì–¸íŠ¸ì˜ underlying client ì ‘ê·¼)
        if hasattr(client, "client"):
            # Instructor ê°ì²´ì¸ ê²½ìš° underlying client ì‚¬ìš©
            underlying_client = client.client
            underlying_client.chat.completions.create(**params)
        else:
            # ì¼ë°˜ OpenAI í´ë¼ì´ì–¸íŠ¸ì¸ ê²½ìš°
            client.chat.completions.create(**params)

        print(
            "âš ï¸ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Context limitì´ ì˜ˆìƒë³´ë‹¤ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )

    except Exception as e:
        analysis = tester.analyze_error(
            "openai", model_info["full_name"], model_info["context_limit"], e
        )
        tester.results.append(analysis)

        print("âœ… Context limit ì—ëŸ¬ ìºì¹˜:")
        print(f"   ì—ëŸ¬ íƒ€ì…: {analysis.error_type}")
        print(f"   ì—ëŸ¬ ì½”ë“œ: {analysis.error_code}")
        print(f"   HTTP ìƒíƒœ: {analysis.http_status_code}")
        print(f"   ë©”ì‹œì§€: {analysis.error_message[:100]}...")


def test_anthropic_context_limit_error():
    """Anthropic APIì˜ context limit ì—ëŸ¬ë¥¼ ì§ì ‘ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    if not os.getenv("ANTHROPIC_API_KEY"):
        pytest.skip("ANTHROPIC_API_KEY not found")

    model_info: ModelInfoDict = {
        "full_name": "claude-sonnet-4-20250514",
        "aliases": ["claude-sonnet-4"],
        "description": "Context limit test",
        "provider": ModelProvider.ANTHROPIC,
        "params": {"temperature": 0.0},
        "thinking_mode": False,
        "pricing": {"input": 3.0, "output": 15.0, "description": "Test"},
        "context_limit": 200000,
    }

    print("\n=== Anthropic Context Limit í…ŒìŠ¤íŠ¸ ===")
    print(f"ëª¨ë¸: {model_info['full_name']}")
    print(f"Context Limit: {model_info['context_limit']:,} tokens")

    try:
        gateway = GatewayFactory.create(model_info["full_name"])
        # Anthropic ì „ìš© ë©”ì„œë“œ ì‚¬ìš©í•˜ì—¬ systemê³¼ messages ë¶„ë¦¬
        system_content, messages = tester.create_anthropic_oversized_content(
            model_info["context_limit"]
        )

        # ì§ì ‘ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = gateway._create_client()

        print(f"í”„ë¡¬í”„íŠ¸ í¬ê¸°: ì•½ {len(str(messages)) // 4:,} tokens ì¶”ì •")

        # Anthropic API ì§ì ‘ í˜¸ì¶œ - systemì„ ë³„ë„ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬
        if hasattr(client, "client"):
            # Instructor ê°ì²´ì¸ ê²½ìš° underlying client ì‚¬ìš©
            underlying_client = client.client
            underlying_client.messages.create(
                model=model_info["full_name"],
                max_tokens=1000,
                system=system_content,
                messages=messages,
                temperature=0.0,
            )
        else:
            # ì¼ë°˜ Anthropic í´ë¼ì´ì–¸íŠ¸ì¸ ê²½ìš°
            client.messages.create(
                model=model_info["full_name"],
                max_tokens=1000,
                system=system_content,
                messages=messages,
                temperature=0.0,
            )

        print(
            "âš ï¸ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Context limitì´ ì˜ˆìƒë³´ë‹¤ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )

    except Exception as e:
        analysis = tester.analyze_error(
            "anthropic", model_info["full_name"], model_info["context_limit"], e
        )
        tester.results.append(analysis)

        print("âœ… Context limit ì—ëŸ¬ ìºì¹˜:")
        print(f"   ì—ëŸ¬ íƒ€ì…: {analysis.error_type}")
        print(f"   ì—ëŸ¬ ì½”ë“œ: {analysis.error_code}")
        print(f"   HTTP ìƒíƒœ: {analysis.http_status_code}")
        print(f"   ë©”ì‹œì§€: {analysis.error_message[:100]}...")


def test_google_context_limit_error():
    """Google APIì˜ context limit ì—ëŸ¬ë¥¼ ì§ì ‘ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    if not os.getenv("GEMINI_API_KEY"):
        pytest.skip("GEMINI_API_KEY not found")

    model_info: ModelInfoDict = {
        "full_name": "gemini-2.5-flash",
        "aliases": [],
        "description": "Context limit test",
        "provider": ModelProvider.GOOGLE,
        "params": {"temperature": 0.0},
        "thinking_mode": False,
        "pricing": {"input": 0.15, "output": 0.6, "description": "Test"},
        "context_limit": 1048576,
    }

    print("\n=== Google Context Limit í…ŒìŠ¤íŠ¸ ===")
    print(f"ëª¨ë¸: {model_info['full_name']}")
    print(f"Context Limit: {model_info['context_limit']:,} tokens")

    try:
        gateway = GatewayFactory.create(model_info["full_name"])
        messages = tester.create_oversized_messages(model_info["context_limit"])

        # ì§ì ‘ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° í˜¸ì¶œ
        client = gateway._create_client()
        params = gateway._create_request_params(messages)

        print(f"í”„ë¡¬í”„íŠ¸ í¬ê¸°: ì•½ {len(str(messages)) // 4:,} tokens ì¶”ì •")

        # ì§ì ‘ API í˜¸ì¶œ
        client.models.generate_content(**params)

        print(
            "âš ï¸ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Context limitì´ ì˜ˆìƒë³´ë‹¤ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )

    except Exception as e:
        analysis = tester.analyze_error(
            "google", model_info["full_name"], model_info["context_limit"], e
        )
        tester.results.append(analysis)

        print("âœ… Context limit ì—ëŸ¬ ìºì¹˜:")
        print(f"   ì—ëŸ¬ íƒ€ì…: {analysis.error_type}")
        print(f"   ì—ëŸ¬ ì½”ë“œ: {analysis.error_code}")
        print(f"   HTTP ìƒíƒœ: {analysis.http_status_code}")
        print(f"   ë©”ì‹œì§€: {analysis.error_message[:100]}...")


@pytest.mark.parametrize("model_name", ["qwen3-coder", "kimi-k2", "deepseek-r1-0528"])
def test_openrouter_models_context_limit_error(models_config, model_name):
    """OpenRouter ëª¨ë¸ë“¤ì˜ ì‹¤ì œ context limitì„ ì‚¬ìš©í•œ ì—ëŸ¬ í…ŒìŠ¤íŠ¸"""
    if not os.getenv("OPENROUTER_API_KEY"):
        pytest.skip("OPENROUTER_API_KEY not found")

    # models.ymlì—ì„œ ì‹¤ì œ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    if model_name not in models_config:
        pytest.skip(f"Model {model_name} not found in models.yml")

    model_config = models_config[model_name]

    # ModelInfoDict í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (provider ë¬¸ìì—´ì„ enumìœ¼ë¡œ ë³€í™˜)
    provider_map = {
        "openai": ModelProvider.OPENAI,
        "anthropic": ModelProvider.ANTHROPIC,
        "google": ModelProvider.GOOGLE,
        "openrouter": ModelProvider.OPENROUTER,
    }

    model_info: ModelInfoDict = {
        "full_name": model_config["full_name"],
        "aliases": model_config.get("aliases", []),
        "description": model_config["description"],
        "provider": provider_map[model_config["provider"]],
        "params": model_config["params"],
        "thinking_mode": model_config.get("thinking_mode", False),
        "pricing": model_config["pricing"],
        "context_limit": model_config["context_limit"],  # ì‹¤ì œ context limit ì‚¬ìš©
    }

    # OpenRouter ëª¨ë¸ë§Œ openrouter_name ì¶”ê°€
    if "openrouter_name" in model_config:
        model_info["openrouter_name"] = model_config["openrouter_name"]

    print(f"\n=== OpenRouter ({model_name}) Context Limit í…ŒìŠ¤íŠ¸ ===")
    print(f"ëª¨ë¸: {model_info['full_name']}")
    print(f"ì‹¤ì œ Context Limit: {model_info['context_limit']:,} tokens")

    try:
        gateway = GatewayFactory.create(model_info["full_name"])
        messages = tester.create_oversized_messages(model_info["context_limit"])

        # ì§ì ‘ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° í˜¸ì¶œ
        client = gateway._create_client()
        params = gateway._create_request_params(messages)

        print(f"í”„ë¡¬í”„íŠ¸ í¬ê¸°: ì•½ {len(str(messages)) // 4:,} tokens ì¶”ì •")

        # ì§ì ‘ API í˜¸ì¶œ (OpenRouter ì „ìš© í´ë¼ì´ì–¸íŠ¸)
        with client as http_client:
            http_client.create_completion(**params)

        print(
            "âš ï¸ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Context limitì´ ì˜ˆìƒë³´ë‹¤ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )

    except Exception as e:
        analysis = tester.analyze_error(
            "openrouter", model_info["full_name"], model_info["context_limit"], e
        )
        tester.results.append(analysis)

        print("âœ… Context limit ì—ëŸ¬ ìºì¹˜:")
        print(f"   ì—ëŸ¬ íƒ€ì…: {analysis.error_type}")
        print(f"   ì—ëŸ¬ ì½”ë“œ: {analysis.error_code}")
        print(f"   HTTP ìƒíƒœ: {analysis.http_status_code}")
        print(f"   ë©”ì‹œì§€: {analysis.error_message[:100]}...")


def test_save_analysis_results():
    """ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³  ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n=== Context Limit ì—ëŸ¬ ë¶„ì„ ì™„ë£Œ ===")

    if not tester.results:
        print("âŒ ë¶„ì„ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   API keyë¥¼ í™•ì¸í•˜ê³  ë‹¤ë¥¸ í…ŒìŠ¤íŠ¸ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    # ê²°ê³¼ ì €ì¥
    tester.save_results()

    # ìš”ì•½ ì¶œë ¥
    print("\nğŸ“Š ë¶„ì„ ìš”ì•½:")
    for result in tester.results:
        print(f"\nğŸ” {result.provider.upper()} - {result.model}")
        print(f"   ì—ëŸ¬ íƒ€ì…: {result.error_type}")
        print(f"   ì—ëŸ¬ ì½”ë“œ: {result.error_code}")
        print(f"   HTTP ìƒíƒœ: {result.http_status_code}")
        print(f"   ë©”ì‹œì§€: {result.error_message[:150]}...")


if __name__ == "__main__":
    print("Context Limit Error Analysis Test")
    print("ì‹¤í–‰ ë°©ë²•: pytest tests/test_context_limit_error_analysis.py -v -s")
