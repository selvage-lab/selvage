# Selvage 컨텍스트 수준 최적화 연구 요청서

## 📋 프로젝트 개요

### Selvage 프로젝트
- **목적**: LLM 기반 자동화된 코드 리뷰 도구
- **주요 기능**: Git diff 분석 → LLM 분석 → 코드 리뷰 보고서 생성
- **지원 모델**: GPT-4o, Claude-3.5-Sonnet, Gemini-Pro 등 (컨텍스트 제한: 32K~128K 토큰)
- **언어**: Python 3.10+, 타겟 언어: Python, JavaScript, TypeScript, Java 등

### 현재 아키텍처
```
CLI → Git Diff 파싱 → 파일 컨텍스트 로딩 → LLM 분석 → 리뷰 결과 출력
      (parser.py)    (file_utils.py)      (llm_gateway/)
```

#### 핵심 컴포넌트
1. **DiffParser** (`selvage/src/diff_parser/`)
   - `FileDiff`: 파일별 변경사항 (filename, file_content, hunks, additions, deletions)
   - `Hunk`: 개별 변경 블록 (header, content, before_code, after_code, line_numbers)

2. **현재 컨텍스트 로딩 방식**
   ```python
   # 현재: 이진 선택
   if use_full_context:  # not diff_only
       file_content = load_file_content(filename, repo_path)  # 전체 파일
   else:
       file_content = None  # 변경사항만
   ```

## 🚨 연구 배경 및 문제점

### 현재 시스템의 한계
1. **비효율적 토큰 사용**: 1-2줄 변경에도 수천 줄 파일 전체 전송
2. **비용 문제**: GPT-4o 기준 $2.50/1M input tokens → 불필요한 비용 발생
3. **성능 저하**: 과도한 컨텍스트로 인한 LLM 집중력 분산
4. **사용자 경험**: 매번 diff-only 옵션 수동 선택의 불편함

### 목표
**변경사항의 특성에 따른 자동화된 적응형 컨텍스트 제공**

## 🔬 연구 주제

### 주제 1: 컨텍스트 수준 분류 기준 최적화

#### 현재 제안된 분류 기준
```python
def determine_context_level(file_diff: FileDiff) -> ContextLevel:
    total_changes = file_diff.additions + file_diff.deletions
    
    if total_changes <= 2:
        return ContextLevel.DIFF_ONLY           # 변경된 라인만
    elif total_changes <= 10:
        return ContextLevel.SURROUNDING         # 변경 라인 + 주변 컨텍스트  
    elif total_changes <= 50:
        return ContextLevel.FUNCTION_BLOCK      # 함수/클래스 블록 전체
    else:
        return ContextLevel.FULL_FILE          # 전체 파일
```

#### 연구 질문
1. **임계값 최적화**: 2, 10, 50이라는 숫자가 적절한가?
2. **다차원 기준**: 단순 변경 라인 수 외에 고려할 요소들
   - 파일 크기 대비 변경 비율
   - 변경 타입 (추가/삭제/수정)
   - 파일 언어/확장자
   - Hunk 개수 및 분산도
   - 변경 위치 (파일 시작/중간/끝)
3. **동적 조정**: 파일 특성에 따른 임계값 조정 필요성
4. **실제 데이터 기반 검증**: 다양한 오픈소스 프로젝트 diff 데이터 분석

### 주제 2: 컨텍스트 추출 구현 방식 비교 분석

#### 구현 옵션 1: 자체 구현
```python
def extract_surrounding_lines(filename: str, repo_path: str, hunks: list[Hunk], context_lines: int = 3) -> str:
    """변경된 라인 주변 N줄 추출"""
    # 단순 라인 번호 기반 주변 텍스트 추출
    
def extract_function_blocks(filename: str, repo_path: str, hunks: list[Hunk]) -> str:
    """변경이 포함된 함수/클래스 블록 추출 (들여쓰기 기반 휴리스틱)"""
    # Python: def/class 키워드 + 들여쓰기 분석
    # JavaScript: function/class 키워드 + 브레이스 매칭
```

#### 구현 옵션 2: grep-ast 활용
- **장점**: tree-sitter 기반 정확한 AST 파싱
- **우려사항**: 외부 의존성, 메모리 사용량, 성능 오버헤드
- **활용 방안**: subprocess 호출 vs Python 라이브러리 통합

#### 구현 옵션 3: 대안 도구 분석
- **tree-sitter-languages**: 직접 tree-sitter 활용
- **ast 모듈**: Python 내장 AST 파서 (Python 파일 한정)
- **Language Server Protocol (LSP)**: 언어별 서버 활용
- **Rope, Jedi** 등: Python 특화 코드 분석 도구

#### 연구 질문
1. **정확도 비교**: 각 방식의 함수/클래스 블록 추출 정확도
2. **성능 벤치마크**: 처리 시간, 메모리 사용량, CPU 사용률
3. **언어별 지원**: 다양한 프로그래밍 언어에서의 동작 여부
4. **에러 처리**: 구문 오류가 있는 파일 처리 능력
5. **유지보수성**: 코드 복잡도, 외부 의존성 관리

## 📊 연구 방법론 제안

### 데이터셋 구성
1. **다양한 프로젝트**: Python, JavaScript, Java, Go, Rust 프로젝트들
2. **변경 크기별 샘플**: 1줄~1000줄 변경사항 균등 분포
3. **실제 커밋 데이터**: GitHub의 인기 오픈소스 프로젝트 커밋 히스토리

### 평가 지표
1. **토큰 효율성**: 컨텍스트 크기 vs 리뷰 품질 비율
2. **리뷰 품질**: LLM이 생성한 리뷰의 정확성, 유용성
3. **처리 성능**: 파싱 시간, 메모리 사용량
4. **비용 효과**: API 호출 비용 절감 효과

### 실험 설계
1. **A/B 테스트**: 현재 방식 vs 제안 방식 비교
2. **민감도 분석**: 임계값 변경에 따른 성능 변화
3. **언어별 분석**: 프로그래밍 언어별 최적 전략

## 🔧 기술적 제약사항

### Selvage 현재 아키텍처 제약
- **Python 3.10+** 환경
- **의존성 최소화** 정책 (현재 11개 라이브러리)
- **CLI 도구**로서의 빠른 실행 속도 요구
- **Git 저장소** 내에서만 동작

### LLM 모델별 제약
- **토큰 제한**: 32K~128K (모델별 상이)
- **비용 고려**: 입력 토큰당 $0.0025~0.006
- **언어 지원**: 각 LLM의 프로그래밍 언어 이해도 차이

## 🎯 기대 연구 결과

### 주요 산출물
1. **최적 임계값 권고안**: 실데이터 기반 컨텍스트 수준 분류 기준
2. **구현 방식 비교 보고서**: 정확도, 성능, 유지보수성 종합 분석
3. **언어별 최적화 전략**: 프로그래밍 언어별 맞춤 컨텍스트 추출 방법
4. **구현 가이드**: 선택된 방식의 단계별 구현 계획

### 정량적 목표
- **토큰 사용량**: 현재 대비 40-70% 절감
- **처리 속도**: 파싱 오버헤드 < 200ms
- **리뷰 품질**: 현재 수준 유지 또는 향상
- **메모리 사용량**: 추가 메모리 사용 < 100MB

## 🔍 연구 질문 상세

### 1. 컨텍스트 수준 분류 최적화

#### 1.1 기본 임계값 검증
> "2, 10, 50이라는 변경 라인 수 기준이 실제 코드 리뷰 품질과 비용 효율성 측면에서 최적인가?"

**세부 연구 포인트:**
- 1,000개 이상의 실제 커밋 데이터에서 변경 라인 수 분포 분석
- 각 임계값에서 LLM 리뷰 품질 측정 (정확성, 완성도, 유용성)
- 토큰 사용량과 API 비용 분석
- 프로그래밍 언어별 최적 임계값 차이 확인

#### 1.2 다차원 분류 기준 개발
> "단순 변경 라인 수 외에 어떤 요소들을 고려해야 더 정확한 컨텍스트 수준을 결정할 수 있는가?"

**후보 요소들:**
- **파일 크기 대비 변경 비율**: 100줄 파일의 10줄 변경 vs 1000줄 파일의 10줄 변경
- **변경 타입 가중치**: 순수 추가(+) vs 삭제(-) vs 수정(+/-)의 영향도
- **Hunk 분산도**: 한 곳 집중 변경 vs 여러 곳 분산 변경
- **변경 위치**: 파일 시작부(imports) vs 중간부(logic) vs 끝부(tests)
- **코드 복잡도**: 중첩 깊이, 함수/클래스 경계 횡단 여부

#### 1.3 언어별 최적화
> "Python, JavaScript, Java 등 언어별로 다른 컨텍스트 전략이 필요한가?"

**언어별 특성:**
- **Python**: 들여쓰기 기반, import 구조의 중요성
- **JavaScript**: 클로저, 비동기 패턴의 복잡성  
- **Java**: 패키지 구조, 상속 관계의 중요성
- **Go**: 간결함, 인터페이스 중심 설계

### 2. 컨텍스트 추출 구현 방식 심화 분석

#### 2.1 grep-ast 도구 심층 분석
> "Aider-AI의 grep-ast를 Selvage에 통합했을 때의 실제 효과와 트레이드오프는?"

**분석 포인트:**
- **정확도 테스트**: 복잡한 중첩 구조, 람다 함수, 데코레이터 등에서의 블록 추출 정확도
- **성능 벤치마크**: 대용량 파일(10K+ 라인)에서의 처리 시간
- **메모리 프로파일링**: tree-sitter 파서 로딩 및 AST 구축 메모리 사용량
- **의존성 영향**: 바이너리 크기, 설치 복잡도, 크로스 플랫폼 호환성

#### 2.2 자체 구현 vs 외부 도구 비교
> "간단한 휴리스틱 기반 자체 구현이 복잡한 AST 기반 도구보다 실용적일 수 있는가?"

**비교 테스트 설계:**
```python
# 자체 구현 예시 (휴리스틱)
def extract_python_function_block(content: str, target_line: int) -> str:
    """들여쓰기와 키워드 기반 함수 블록 추출"""
    lines = content.split('\n')
    # def/class 키워드 찾기 + 들여쓰기 레벨 추적
    
# AST 기반 예시 (tree-sitter)
def extract_ast_function_block(content: str, target_line: int) -> str:
    """AST 파싱 기반 정확한 함수 블록 추출"""
    # tree-sitter로 AST 구축 + 노드 탐색
```

**평가 기준:**
- **정확도**: 100개 테스트 케이스에서 올바른 블록 추출 비율
- **성능**: 1000회 반복 실행 평균 시간
- **견고성**: 구문 오류, 비표준 코딩 스타일에서의 동작
- **유지보수성**: 코드 복잡도, 새로운 언어 추가 난이도

#### 2.3 대안 도구 조사
> "grep-ast 외에 더 적합한 대안이 있는가?"

**후보 도구들:**
1. **tree-sitter-cli**: 직접 바이너리 호출
2. **py-tree-sitter**: Python 바인딩 직접 사용
3. **Language Server Protocol**: pylsp, typescript-language-server 등
4. **언어별 전용 도구**: 
   - Python: `ast`, `rope`, `jedi`
   - JavaScript: `@babel/parser`, `acorn`
   - Java: `JavaParser`

**평가 프레임워크:**
- **통합 용이성**: Selvage 아키텍처와의 호환성
- **성능**: 콜드 스타트, 연속 처리 시간
- **정확도**: 복잡한 코드 구조 처리 능력
- **확장성**: 새로운 언어 지원 추가 방법

## 🧪 실험 데이터셋 제안

### 실제 프로젝트 커밋 데이터
```
1. Python 프로젝트
   - Django (웹 프레임워크)
   - NumPy (과학 계산)
   - FastAPI (API 프레임워크)
   
2. JavaScript 프로젝트  
   - React (UI 라이브러리)
   - Express.js (서버 프레임워크)
   - Lodash (유틸리티 라이브러리)

3. Java 프로젝트
   - Spring Boot (엔터프라이즈 프레임워크)
   - Apache Kafka (스트리밍 플랫폼)
```

### 테스트 케이스 분류
```
변경 크기별:
- 극소형 (1-2줄): 변수명 변경, 상수 수정
- 소형 (3-10줄): 버그 수정, 간단한 기능 추가  
- 중형 (11-50줄): 함수 추가, 로직 리팩터링
- 대형 (51+줄): 클래스 추가, 아키텍처 변경

변경 타입별:
- 순수 추가: 새 함수/클래스 추가
- 순수 삭제: 기능 제거, 데드 코드 삭제
- 혼합 수정: 기존 로직 변경
```

## 🎯 연구 결과 활용 방안

### 1단계: 프로토타입 구현 (2주)
- 연구 결과 기반 최적 임계값 적용
- 선택된 구현 방식으로 POC 개발
- 기본 테스트 케이스 검증

### 2단계: 성능 최적화 (1주)
- 병목 지점 분석 및 개선
- 메모리 사용량 최적화
- 에러 처리 강화

### 3단계: 운영 환경 적용 (1주)
- 기존 diff-only 옵션 마이그레이션
- 사용자 피드백 수집 체계 구축
- 모니터링 및 로깅 강화

## 📈 성공 지표

### 정량적 지표
- **토큰 절약률**: 현재 대비 40% 이상 절감
- **처리 성능**: 추가 오버헤드 200ms 이하
- **정확도**: 함수 블록 추출 정확도 95% 이상
- **메모리 효율**: 추가 메모리 사용량 100MB 이하

### 정성적 지표
- **사용자 만족도**: diff-only 옵션 제거로 인한 편의성 향상
- **리뷰 품질**: LLM 응답의 관련성 및 유용성 유지/향상
- **개발 효율성**: 자동화 수준 향상으로 인한 개발 생산성 증대

---

이 연구를 통해 Selvage의 컨텍스트 최적화를 달성하고, LLM 기반 코드 리뷰 도구의 새로운 표준을 제시할 수 있기를 기대합니다.